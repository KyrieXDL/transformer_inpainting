2022-07-24 02:09:09,645 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_multiscale_external_fpn_v2/pytorch_model.bin', decoder_arch='multiscale_transformer', decoder_depth=12, decoder_embed_dim=768, depth=6, device_ids='0', embed_dim=768, epochs=30, flag='mae_multiscale_external_fpn_v2', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, model_save_path='./saved_models/mae_multiscale_external_fpn_v2', num_workers=4, output_dir='./output/logs', patch_size=16, phase='val', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=True, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-24 02:09:45,077 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_multiscale_external_fpn_v2/pytorch_model.bin', decoder_arch='multiscale_transformer', decoder_depth=12, decoder_embed_dim=768, depth=6, device_ids='0', embed_dim=768, epochs=30, flag='mae_multiscale_external_fpn_v2', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, model_save_path='./saved_models/mae_multiscale_external_fpn_v2', num_workers=4, output_dir='./output/logs', patch_size=16, phase='val', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=True, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-24 02:09:58,636 - utils - INFO - load ckpt from ./saved_models/mae_multiscale_external_fpn_v2/pytorch_model.bin
2022-07-24 02:09:58,637 - utils - INFO - msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['decoder.blocks.4.attn.pool_q.weight', 'decoder.blocks.4.attn.norm_q.weight', 'decoder.blocks.4.attn.norm_q.bias', 'decoder.blocks.4.attn.pool_k.weight', 'decoder.blocks.4.attn.norm_k.weight', 'decoder.blocks.4.attn.norm_k.bias', 'decoder.blocks.4.attn.pool_v.weight', 'decoder.blocks.4.attn.norm_v.weight', 'decoder.blocks.4.attn.norm_v.bias', 'decoder.blocks.8.attn.pool_q.weight', 'decoder.blocks.8.attn.norm_q.weight', 'decoder.blocks.8.attn.norm_q.bias', 'decoder.blocks.8.attn.pool_k.weight', 'decoder.blocks.8.attn.norm_k.weight', 'decoder.blocks.8.attn.norm_k.bias', 'decoder.blocks.8.attn.pool_v.weight', 'decoder.blocks.8.attn.norm_v.weight', 'decoder.blocks.8.attn.norm_v.bias'])
2022-07-24 02:14:07,248 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_multiscale_external_fpn_v2/pytorch_model.bin', decoder_arch='multiscale_transformer', decoder_depth=12, decoder_embed_dim=768, depth=6, device_ids='0', embed_dim=768, epochs=30, flag='mae_multiscale_external_fpn_v2', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, model_save_path='./saved_models/mae_multiscale_external_fpn_v2', num_workers=4, output_dir='./output/logs', patch_size=16, phase='val', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=True, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-24 02:14:19,926 - utils - INFO - load ckpt from ./saved_models/mae_multiscale_external_fpn_v2/pytorch_model.bin
2022-07-24 02:14:19,926 - utils - INFO - msg: <All keys matched successfully>
2022-07-24 02:34:47,367 - utils - INFO - epoch: 0, loss: 1.3044946193695068, mae: 5.857902933729874, ssim: 0.9672179555858007, psnr: 28.542256663454665
