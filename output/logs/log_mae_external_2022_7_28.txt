2022-07-28 08:42:41,629 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=768, decoder_num_heads=12, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-28 08:44:15,055 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=12, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-28 08:44:27,273 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 08:44:27,273 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 08:49:26,605 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=8, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-28 08:49:38,676 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 08:49:38,676 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 09:08:35,307 - utils - INFO - epoch: 0, loss: 2.995415687561035, mae: 7.037578825364556, ssim: 0.9282749649932442, psnr: 24.83082955983197
2022-07-28 09:12:24,634 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=8, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-28 09:12:36,673 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 09:12:36,673 - utils - INFO - msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['decoder.8.norm1.weight', 'decoder.8.norm1.bias', 'decoder.8.attn.qkv.weight', 'decoder.8.attn.qkv.bias', 'decoder.8.attn.proj.weight', 'decoder.8.attn.proj.bias', 'decoder.8.norm2.weight', 'decoder.8.norm2.bias', 'decoder.8.mlp.fc1.weight', 'decoder.8.mlp.fc1.bias', 'decoder.8.mlp.fc2.weight', 'decoder.8.mlp.fc2.bias', 'decoder.9.norm1.weight', 'decoder.9.norm1.bias', 'decoder.9.attn.qkv.weight', 'decoder.9.attn.qkv.bias', 'decoder.9.attn.proj.weight', 'decoder.9.attn.proj.bias', 'decoder.9.norm2.weight', 'decoder.9.norm2.bias', 'decoder.9.mlp.fc1.weight', 'decoder.9.mlp.fc1.bias', 'decoder.9.mlp.fc2.weight', 'decoder.9.mlp.fc2.bias', 'decoder.10.norm1.weight', 'decoder.10.norm1.bias', 'decoder.10.attn.qkv.weight', 'decoder.10.attn.qkv.bias', 'decoder.10.attn.proj.weight', 'decoder.10.attn.proj.bias', 'decoder.10.norm2.weight', 'decoder.10.norm2.bias', 'decoder.10.mlp.fc1.weight', 'decoder.10.mlp.fc1.bias', 'decoder.10.mlp.fc2.weight', 'decoder.10.mlp.fc2.bias', 'decoder.11.norm1.weight', 'decoder.11.norm1.bias', 'decoder.11.attn.qkv.weight', 'decoder.11.attn.qkv.bias', 'decoder.11.attn.proj.weight', 'decoder.11.attn.proj.bias', 'decoder.11.norm2.weight', 'decoder.11.norm2.bias', 'decoder.11.mlp.fc1.weight', 'decoder.11.mlp.fc1.bias', 'decoder.11.mlp.fc2.weight', 'decoder.11.mlp.fc2.bias'])
2022-07-28 09:14:21,577 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks', valid_weight=1.0, warmup_steps=1000)
2022-07-28 09:14:33,803 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 09:14:33,803 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 09:33:40,996 - utils - INFO - epoch: 0, loss: 1.2022908926010132, mae: 4.9340633008664, ssim: 0.9709653631370501, psnr: 29.13311901161697
2022-07-28 09:34:18,695 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks_7_8', valid_weight=1.0, warmup_steps=1000)
2022-07-28 09:34:31,713 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 09:34:31,713 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 09:53:35,205 - utils - INFO - epoch: 0, loss: 1.530991792678833, mae: 6.862709342908925, ssim: 0.9411861258847742, psnr: 25.540052780591125
2022-07-28 10:25:40,233 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='2', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks_8_9', valid_weight=1.0, warmup_steps=1000)
2022-07-28 10:25:54,784 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 10:25:54,785 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 10:26:07,857 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='3', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks_8_9', valid_weight=1.0, warmup_steps=1000)
2022-07-28 10:26:20,898 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 10:26:20,898 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 10:45:17,757 - utils - INFO - epoch: 0, loss: 2.1546530723571777, mae: 8.597158710617942, ssim: 0.9132805612699408, psnr: 23.717726013847813
2022-07-28 14:19:47,109 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='3', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks_6_7', valid_weight=1.0, warmup_steps=1000)
2022-07-28 14:20:00,067 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 14:20:00,068 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 14:39:33,954 - utils - INFO - epoch: 0, loss: 0.8697375655174255, mae: 5.616606269524548, ssim: 0.9605159190373451, psnr: 27.400586851016254
2022-07-28 15:17:26,399 - utils - INFO - Namespace(batchsize=64, checkpoint='./saved_models/mae_external/pytorch_model.bin', decoder_arch='transformer', decoder_depth=12, decoder_embed_dim=512, decoder_num_heads=16, depth=6, device_ids='3', embed_dim=768, epochs=30, flag='mae_external', fpn_type='2d', hole_weight=6.0, kv_downsample_layers='[4, 8]', lr=5e-05, mae_hole_weight=1, mae_valid_weight=0, model_save_path='./saved_models/mae_external', num_heads=12, num_workers=8, output_dir='./output/logs', patch_size=16, phase='val', pool_type='interpolate', prefetch=6, q_downsample_layers='[4, 8]', schedule_type='', seed=42, train_data_path='../data/celeba_train_data.txt', train_mask_path='../data/train_masks', use_adv=False, use_external_mask=True, use_fpn_loss=False, use_mae_loss=False, use_norm_pred=False, use_pconv=False, use_pyramid=False, use_random_mask=False, val_data_path='../data/celeba_val_data.txt', val_mask_path='../data/val_masks_5_6', valid_weight=1.0, warmup_steps=1000)
2022-07-28 15:17:38,407 - utils - INFO - load ckpt from ./saved_models/mae_external/pytorch_model.bin
2022-07-28 15:17:38,408 - utils - INFO - msg: <All keys matched successfully>
2022-07-28 15:37:04,295 - utils - INFO - epoch: 0, loss: 0.5879365801811218, mae: 4.667383039571743, ssim: 0.9740378165476012, psnr: 29.43373601198057
